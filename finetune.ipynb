{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT微调"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-12-30T09:28:49.763777Z",
     "iopub.status.busy": "2023-12-30T09:28:49.762957Z",
     "iopub.status.idle": "2023-12-30T09:28:49.767850Z",
     "shell.execute_reply": "2023-12-30T09:28:49.767179Z",
     "shell.execute_reply.started": "2023-12-30T09:28:49.763745Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "BERT_PATH='./BERT_CCPoem_v1/BERT_CCPoem_v1/'\n",
    "from sklearn.metrics import roc_auc_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T09:17:14.808437Z",
     "iopub.status.busy": "2023-12-30T09:17:14.807944Z",
     "iopub.status.idle": "2023-12-30T09:17:15.099157Z",
     "shell.execute_reply": "2023-12-30T09:17:15.098232Z",
     "shell.execute_reply.started": "2023-12-30T09:17:14.808405Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "def report_gpu():\n",
    "   print(torch.cuda.list_gpu_processes())\n",
    "   gc.collect()\n",
    "   torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T09:17:16.392859Z",
     "iopub.status.busy": "2023-12-30T09:17:16.392396Z",
     "iopub.status.idle": "2023-12-30T09:17:16.457823Z",
     "shell.execute_reply": "2023-12-30T09:17:16.457124Z",
     "shell.execute_reply.started": "2023-12-30T09:17:16.392828Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('./BERT_CCPoem_v1/dataprocess/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T09:17:17.126110Z",
     "iopub.status.busy": "2023-12-30T09:17:17.125647Z",
     "iopub.status.idle": "2023-12-30T09:17:17.133900Z",
     "shell.execute_reply": "2023-12-30T09:17:17.133258Z",
     "shell.execute_reply.started": "2023-12-30T09:17:17.126077Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>translation</th>\n",
       "      <th>choices</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>诗人啊，你竟像在遥远的地方站立船头。</td>\n",
       "      <td>行人初上木兰舟|骚人遥驻木兰舟|有人独上木兰舟|行人迢递木兰舟</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>他的双眼眼瞳碧绿而有光，头发金黄而弯曲，两鬓呈红色。</td>\n",
       "      <td>绿玉觜攒鸡脑破，玄金爪擘兔心开。|翅金肉白顶红麻，项糁毛青腿少瑕。|头似珊瑚项班红，翅如金箔...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>清晨还是西北风。</td>\n",
       "      <td>清晨西北转|河岳西来转|凌晨从北固|西北转银潢</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>柴烟中红星乱闪。</td>\n",
       "      <td>流星紫入烟|红光生紫烟|乱荷红带紫|红星乱紫烟</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>在他们身边痛哭的只有尚未省事的儿郎。</td>\n",
       "      <td>狂叫唯童儿|狂呼造化儿|儿童趁欲狂|学叫笑儿娱</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  translation  \\\n",
       "0          诗人啊，你竟像在遥远的地方站立船头。   \n",
       "1  他的双眼眼瞳碧绿而有光，头发金黄而弯曲，两鬓呈红色。   \n",
       "2                    清晨还是西北风。   \n",
       "3                    柴烟中红星乱闪。   \n",
       "4          在他们身边痛哭的只有尚未省事的儿郎。   \n",
       "\n",
       "                                             choices  answer  \n",
       "0                    行人初上木兰舟|骚人遥驻木兰舟|有人独上木兰舟|行人迢递木兰舟       1  \n",
       "1  绿玉觜攒鸡脑破，玄金爪擘兔心开。|翅金肉白顶红麻，项糁毛青腿少瑕。|头似珊瑚项班红，翅如金箔...       3  \n",
       "2                            清晨西北转|河岳西来转|凌晨从北固|西北转银潢       0  \n",
       "3                            流星紫入烟|红光生紫烟|乱荷红带紫|红星乱紫烟       3  \n",
       "4                            狂叫唯童儿|狂呼造化儿|儿童趁欲狂|学叫笑儿娱       0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T09:17:19.979796Z",
     "iopub.status.busy": "2023-12-30T09:17:19.979323Z",
     "iopub.status.idle": "2023-12-30T09:17:20.029553Z",
     "shell.execute_reply": "2023-12-30T09:17:20.028851Z",
     "shell.execute_reply.started": "2023-12-30T09:17:19.979765Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((21778, 1), (21778, 4), (21778, 1))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=df.translation.to_numpy().reshape(-1,1)\n",
    "choices=np.array(df['choices'].str.split('|').tolist())\n",
    "true_label=df.answer.to_numpy().reshape(-1,1)\n",
    "x.shape,choices.shape,true_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T09:17:20.753712Z",
     "iopub.status.busy": "2023-12-30T09:17:20.753237Z",
     "iopub.status.idle": "2023-12-30T09:17:20.769538Z",
     "shell.execute_reply": "2023-12-30T09:17:20.768672Z",
     "shell.execute_reply.started": "2023-12-30T09:17:20.753682Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = np.concatenate((x,choices ), axis=1) #沿着水平方向进行合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-12-30T09:17:28.164426Z",
     "iopub.status.busy": "2023-12-30T09:17:28.163945Z",
     "iopub.status.idle": "2023-12-30T09:17:28.178018Z",
     "shell.execute_reply": "2023-12-30T09:17:28.177386Z",
     "shell.execute_reply.started": "2023-12-30T09:17:28.164395Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15244, 5), (6534, 5), (15244, 1), (6534, 1))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, true_label,test_size=0.3, random_state=10)\n",
    "X_train.shape,X_test.shape,y_train.shape,y_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 微调策略\n",
    "由于数据集的特殊性，一开始我们选择将数据集处理为原文：译文的形式，然后使用BERT进行微调。但是发现这样的微调方式并不好，因为最终模型的训练目标是二者相似，那么相似性可以划分为一个二分类问题，可以利用交叉熵的损失函数去进行训练，但是由于原文：译文的形式，导致训练集中的数据全是正例，模型在训练过程中携带了bias，无法进行训练。\n",
    "\n",
    "在前者的基础上，我们将数据集划分为了 原文：选项（共4个，包含正确的译文）：正确的译文的下标的形式，这样通过计算原文与四个选项的相似度，即可在模型的输出外面再加上一个全连接层，输出的维度是分类数（4），这样损失函数依旧可以选用交叉熵，这样模型也学习了足够多的负例样本模型效果好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T10:38:00.824254Z",
     "iopub.status.busy": "2023-12-30T10:38:00.823782Z",
     "iopub.status.idle": "2023-12-30T10:38:00.831119Z",
     "shell.execute_reply": "2023-12-30T10:38:00.830084Z",
     "shell.execute_reply.started": "2023-12-30T10:38:00.824217Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BertForMultiClassClassificationDataset(torch.utils.data.Dataset):\n",
    "    #定义一个数据集的抽象类\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        #获取长度\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #获取不同位置的元素\n",
    "        text = self.texts[idx].tolist()\n",
    "        label = self.labels[idx]\n",
    "        #使用tokenizer.encode_plus对文本进行分词和编码\n",
    "        tokens = self.tokenizer.batch_encode_plus(\n",
    "            #在batch情况下，需要使用该函数防止维度有问题\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        # print(tokens['input_ids'])\n",
    "        inputs = tokens['input_ids'].squeeze()\n",
    "        attention_mask= tokens['attention_mask'].squeeze() #移除尺寸为1的维度\n",
    "        labels = torch.tensor(label, dtype=torch.long)\n",
    "        return inputs, attention_mask, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-12-30T10:39:33.659205Z",
     "iopub.status.busy": "2023-12-30T10:39:33.658749Z",
     "iopub.status.idle": "2023-12-30T10:39:33.666919Z",
     "shell.execute_reply": "2023-12-30T10:39:33.665851Z",
     "shell.execute_reply.started": "2023-12-30T10:39:33.659171Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BertForMultiClassClassification(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(BertForMultiClassClassification, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(BERT_PATH)\n",
    "        self.dropout = nn.Dropout(0.01)  # 添加一个Dropout层\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)  # 全连接层，将BERT输出映射到分类标签\n",
    "\n",
    "    def forward(self, input_ids,attention_mask):\n",
    "        # input_ids: 输入的令牌序列\n",
    "        # attention_mask: 输入的注意力掩码，用于指定哪些令牌参与注意力计算\n",
    "        input_ids_split = torch.split(input_ids, split_size_or_sections=1, dim=1)\n",
    "        attention_mask_split = torch.split(attention_mask, split_size_or_sections=1, dim=1)\n",
    "        outputs = []\n",
    "        for i in range(len(input_ids_split)):\n",
    "            output= self.bert(input_ids_split[i].squeeze(), attention_mask=attention_mask_split[i].squeeze())\n",
    "            pooled_output = output['pooler_output']\n",
    "\n",
    "            pooled_output = self.dropout(pooled_output)# 使用Dropout层防止过拟合,一定的概率随机将某些神经元的输出设置为零，以防止过拟合。\n",
    "            outputs.append( pooled_output)\n",
    "\n",
    "        outputs = torch.stack(outputs, dim=1) #将五个维度的张量合在一起\n",
    "        translation=outputs[:,0,:].unsqueeze(1)\n",
    "        # 使用余弦相似度译文与四个选项的相似度\n",
    "        similarity_scores =F.softmax( nn.functional.cosine_similarity(outputs[:,1:,:], translation,dim=2))\n",
    "        \n",
    "        \n",
    "\n",
    "        return similarity_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-12-30T10:59:53.213666Z",
     "iopub.status.busy": "2023-12-30T10:59:53.213181Z",
     "iopub.status.idle": "2023-12-30T10:59:53.218051Z",
     "shell.execute_reply": "2023-12-30T10:59:53.217331Z",
     "shell.execute_reply.started": "2023-12-30T10:59:53.213636Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 定义一些超参数\n",
    "max_length = 128 #最大长度\n",
    "batch_size = 32 #批大小\n",
    "num_epochs = 3 #迭代次数\n",
    "learning_rate = 1e-8 #学习率\n",
    "num_classes = 4  #分类数\n",
    "max_batch=16\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T10:39:46.225176Z",
     "iopub.status.busy": "2023-12-30T10:39:46.224716Z",
     "iopub.status.idle": "2023-12-30T10:39:46.243580Z",
     "shell.execute_reply": "2023-12-30T10:39:46.242904Z",
     "shell.execute_reply.started": "2023-12-30T10:39:46.225144Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer=BertTokenizer.from_pretrained(BERT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-30T10:39:47.023981Z",
     "iopub.status.busy": "2023-12-30T10:39:47.023510Z",
     "iopub.status.idle": "2023-12-30T10:39:47.029381Z",
     "shell.execute_reply": "2023-12-30T10:39:47.028604Z",
     "shell.execute_reply.started": "2023-12-30T10:39:47.023951Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 生成相应数据集\n",
    "train_dataset = BertForMultiClassClassificationDataset(X_train, y_train, tokenizer, max_length)\n",
    "test_dataset = BertForMultiClassClassificationDataset(X_test, y_test, tokenizer, max_length)\n",
    "# 加载相应数据集和模型\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-12-30T10:59:56.143249Z",
     "iopub.status.busy": "2023-12-30T10:59:56.142763Z",
     "iopub.status.idle": "2023-12-30T10:59:56.602026Z",
     "shell.execute_reply": "2023-12-30T10:59:56.601256Z",
     "shell.execute_reply.started": "2023-12-30T10:59:56.143214Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 实例化模型\n",
    "model = BertForMultiClassClassification(num_classes) #在models.py中定义的模型，其中使用了bert模型\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() #交叉熵损失函数\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) #使用adamw优化器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-12-30T11:01:17.509678Z",
     "iopub.status.busy": "2023-12-30T11:01:17.509104Z",
     "iopub.status.idle": "2023-12-30T11:13:25.591117Z",
     "shell.execute_reply": "2023-12-30T11:13:25.590355Z",
     "shell.execute_reply.started": "2023-12-30T11:01:17.509646Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_657/4052147173.py:24: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  similarity_scores =F.softmax( nn.functional.cosine_similarity(outputs[:,1:,:], translation,dim=2))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 1.3846672773361206, Training Accuracy: 0.4541, Test Accuracy: 0.6336  F1 Score: 0.6336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_657/4052147173.py:24: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  similarity_scores =F.softmax( nn.functional.cosine_similarity(outputs[:,1:,:], translation,dim=2))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Loss: 1.38505220413208, Training Accuracy: 0.6517, Test Accuracy: 0.6356  F1 Score: 0.6346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_657/4052147173.py:24: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  similarity_scores =F.softmax( nn.functional.cosine_similarity(outputs[:,1:,:], translation,dim=2))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3, Loss: 1.3835978507995605, Training Accuracy: 0.6521, Test Accuracy: 0.6364  F1 Score: 0.6352\n"
     ]
    }
   ],
   "source": [
    "model.train() #设置模型为训练模式\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "for epoch in range(num_epochs):\n",
    "        train_total_correct = 0\n",
    "        train_total_samples = 0\n",
    "        \n",
    "        for input_ids, attention_mask, labels in train_dataloader:\n",
    "                #对每个批次的训练数据进行前向传播、计算损失、反向传播和参数更新\n",
    "\n",
    "                ## 将tensor移动到GPU上\n",
    "                input_ids = input_ids.to(device)\n",
    "                attention_mask = attention_mask.to(device)\n",
    "                labels = labels.to(device).squeeze()\n",
    "                # print(labels)\n",
    "                # print(input_ids.shape,attention_mask.shape,labels.shape)\n",
    "\n",
    "                # 优化器进行初始化\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                # print(outputs.shape)\n",
    "                _,predicted = torch.max(outputs,1)\n",
    "                # print(predicted)\n",
    "                loss = criterion(outputs, labels) #计算损失函数\n",
    "                train_total_correct +=torch.sum(predicted == labels).item()\n",
    "                train_total_samples += labels.size(0)\n",
    "                loss.backward()#反向传播\n",
    "                optimizer.step() #参数更新\n",
    "\n",
    "        train_accuracy = train_total_correct / train_total_samples\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "                #不计算梯度，对test集的元素进行验证\n",
    "                test_total_correct = 0\n",
    "                test_total_samples = 0\n",
    "\n",
    "                for test_input_ids, test_attention_mask, test_labels in test_dataloader:\n",
    "                        test_input_ids = test_input_ids.to(device)\n",
    "                        test_attention_mask = test_attention_mask.to(device)\n",
    "                        test_labels = test_labels.to(device).squeeze()\n",
    "\n",
    "                        test_outputs = model(test_input_ids, test_attention_mask)\n",
    "                        _, test_predicted_labels = torch.max(test_outputs, dim=1)\n",
    "\n",
    "                        test_total_correct += torch.sum(test_predicted_labels ==\n",
    "                                                        test_labels).item()\n",
    "                        test_total_samples += test_labels.size(0)\n",
    "                        all_predictions.extend(test_predicted_labels.cpu().numpy())\n",
    "                        all_labels.extend(test_labels.cpu().numpy())\n",
    "\n",
    "        test_accuracy = test_total_correct / test_total_samples\n",
    "        # auc = roc_auc_score(all_labels, all_predictions,multi_class='ovo')\n",
    "\n",
    "        # 计算F1 Score\n",
    "        f1 = f1_score(all_labels, all_predictions, average='weighted')  # 可以选择适当的average参数\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Training Accuracy: {train_accuracy:.4f}, Test Accuracy: {test_accuracy:.4f}  F1 Score: {f1:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-12-30T11:15:12.112640Z",
     "iopub.status.busy": "2023-12-30T11:15:12.112162Z",
     "iopub.status.idle": "2023-12-30T11:15:12.389309Z",
     "shell.execute_reply": "2023-12-30T11:15:12.388516Z",
     "shell.execute_reply.started": "2023-12-30T11:15:12.112607Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'bert_poem.bin') #保存模型参数至文件中\n",
    "torch.save(model.bert.state_dict(), 'bert_model_params.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
